{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahp70PVLbthm",
        "outputId": "94b71df8-9b22-40d9-945f-6be23c7ba760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'  # Training features\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'  # Test features\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv(train_features_path)"
      ],
      "metadata": {
        "id": "4jFcGmMgcM9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the Random Forest model on the entire scaled training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Test data batch processing\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, all_y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, all_y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF-O9wOhxxp7",
        "outputId": "4995d1b5-7390-4394-98b2-c751daf929bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n",
            "Model training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.7024180967238689\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.55      0.43      0.48       497\n",
            "     Textiles       0.62      0.47      0.53       498\n",
            "         Wood       0.46      0.74      0.56       492\n",
            "    cardboard       0.80      0.90      0.85       548\n",
            "      e-waste       0.95      0.81      0.87       481\n",
            "        glass       0.83      0.75      0.78       504\n",
            "      medical       0.81      0.62      0.70       539\n",
            "        metal       0.79      0.70      0.74       496\n",
            "        paper       0.76      0.83      0.79       550\n",
            "      plastic       0.63      0.77      0.69       523\n",
            "\n",
            "     accuracy                           0.70      5128\n",
            "    macro avg       0.72      0.70      0.70      5128\n",
            " weighted avg       0.72      0.70      0.70      5128\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[214  52 205   0   0   3  10   3   1   9]\n",
            " [ 84 232 170   4   0   0   3   0   4   1]\n",
            " [ 56  43 364  19   0   1   2   1   3   3]\n",
            " [  1   0   3 493   3   1   8   5  28   6]\n",
            " [  0   0   0  18 388   6  22  18  16  13]\n",
            " [  0   2   0  12   4 376   6  22  10  72]\n",
            " [ 32  41  51   7   3   5 332   7  16  45]\n",
            " [  1   2   4  13  10  34  11 346  24  51]\n",
            " [  0   3   0  36   1   1   2  12 456  39]\n",
            " [  0   1   0  14   1  27  13  22  44 401]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4SMX49y11Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the Random Forest model on the entire scaled training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Test data batch processing\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Evaluation for test data\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, all_y_pred))\n",
        "\n",
        "# Confusion Matrix for test data\n",
        "test_conf_matrix = confusion_matrix(y_test, all_y_pred)\n",
        "print(\"\\nTest Confusion Matrix:\\n\", test_conf_matrix)\n",
        "\n",
        "# Evaluation for training data (train confusion matrix)\n",
        "train_pred = model.predict(X_train_scaled)\n",
        "train_conf_matrix = confusion_matrix(y_train, train_pred)\n",
        "print(\"\\nTraining Confusion Matrix:\\n\", train_conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N0DWRecp0Ceb",
        "outputId": "b7e98e38-5592-40fe-8c8c-0d7398a288f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aeefb26aab73>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the Random Forest model on the entire scaled training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model training completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n"
      ],
      "metadata": {
        "id": "wQMXVe9Y12Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "\n",
        "# Get the total number of images (rows in the dataset)\n",
        "total_images = len(train_data)\n",
        "\n",
        "print(f\"Total number of images: {total_images}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbjqGkQw1-Bw",
        "outputId": "852e6b3b-7d1b-4b1a-8bd9-93549221bd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images: 20508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(test_features_path)"
      ],
      "metadata": {
        "id": "lQXzG8yZ2Nh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKAcHsfyz4MJ",
        "outputId": "6064035c-dabf-46bb-db97-74e61fc4b070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "TnOdOrDIf4gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the Random Forest model on the entire scaled training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Test data batch processing\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, all_y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLuyf8P_clnK",
        "outputId": "38445de4-40a9-4bd1-c35c-7baa61bfd3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n",
            "Model training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.7024180967238689\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.55      0.43      0.48       497\n",
            "     Textiles       0.62      0.47      0.53       498\n",
            "         Wood       0.46      0.74      0.56       492\n",
            "    cardboard       0.80      0.90      0.85       548\n",
            "      e-waste       0.95      0.81      0.87       481\n",
            "        glass       0.83      0.75      0.78       504\n",
            "      medical       0.81      0.62      0.70       539\n",
            "        metal       0.79      0.70      0.74       496\n",
            "        paper       0.76      0.83      0.79       550\n",
            "      plastic       0.63      0.77      0.69       523\n",
            "\n",
            "     accuracy                           0.70      5128\n",
            "    macro avg       0.72      0.70      0.70      5128\n",
            " weighted avg       0.72      0.70      0.70      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the Random Forest model on the entire scaled training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Training predictions and classification report\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy}\")\n",
        "print(\"\\nTraining Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Load the test data\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Scale and predict on the test data in batches\n",
        "all_y_pred = []\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Transform using the scaler fitted on training data\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Test evaluation\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy}\")\n",
        "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, all_y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApJGioRJvUGj",
        "outputId": "9a9a2bab-d289-4096-fcea-8cd85d8f3d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n",
            "Model training completed.\n",
            "Training Accuracy: 0.9984396333138288\n",
            "\n",
            "Training Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      1.00      1.00      1989\n",
            "     Textiles       1.00      0.99      0.99      1992\n",
            "         Wood       0.99      1.00      1.00      1966\n",
            "    cardboard       1.00      1.00      1.00      2194\n",
            "      e-waste       1.00      1.00      1.00      1924\n",
            "        glass       1.00      1.00      1.00      2014\n",
            "      medical       1.00      1.00      1.00      2154\n",
            "        metal       1.00      1.00      1.00      1982\n",
            "        paper       1.00      1.00      1.00      2199\n",
            "      plastic       1.00      1.00      1.00      2094\n",
            "\n",
            "     accuracy                           1.00     20508\n",
            "    macro avg       1.00      1.00      1.00     20508\n",
            " weighted avg       1.00      1.00      1.00     20508\n",
            "\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "\n",
            "Test Accuracy: 0.7024180967238689\n",
            "\n",
            "Test Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.55      0.43      0.48       497\n",
            "     Textiles       0.62      0.47      0.53       498\n",
            "         Wood       0.46      0.74      0.56       492\n",
            "    cardboard       0.80      0.90      0.85       548\n",
            "      e-waste       0.95      0.81      0.87       481\n",
            "        glass       0.83      0.75      0.78       504\n",
            "      medical       0.81      0.62      0.70       539\n",
            "        metal       0.79      0.70      0.74       496\n",
            "        paper       0.76      0.83      0.79       550\n",
            "      plastic       0.63      0.77      0.69       523\n",
            "\n",
            "     accuracy                           0.70      5128\n",
            "    macro avg       0.72      0.70      0.70      5128\n",
            " weighted avg       0.72      0.70      0.70      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0QXwbx_CYEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG BOOST"
      ],
      "metadata": {
        "id": "4V92-bVkf7CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the XGBoost model on the entire scaled training data\n",
        "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"XGBoost model training completed.\")\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Test data batch processing\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = xgb_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWMEa9WXf8M7",
        "outputId": "78dc8f3a-5773-414f-ef4c-4e441df833cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [18:28:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.8771450858034321\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.94      0.98      0.96       497\n",
            "     Textiles       0.93      0.93      0.93       498\n",
            "         Wood       0.96      0.96      0.96       492\n",
            "    cardboard       0.85      0.93      0.89       548\n",
            "      e-waste       0.97      0.80      0.88       481\n",
            "        glass       0.88      0.80      0.84       504\n",
            "      medical       0.81      0.92      0.86       539\n",
            "        metal       0.90      0.74      0.81       496\n",
            "        paper       0.83      0.88      0.85       550\n",
            "      plastic       0.76      0.83      0.80       523\n",
            "\n",
            "     accuracy                           0.88      5128\n",
            "    macro avg       0.88      0.88      0.88      5128\n",
            " weighted avg       0.88      0.88      0.88      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desicion tree"
      ],
      "metadata": {
        "id": "kAaBwYPh3RSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    criterion='gini',  # You can change this to 'entropy' for information gain\n",
        "    max_depth=None,  # You can adjust the depth if needed to avoid overfitting\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the Decision Tree model on the entire scaled training data\n",
        "dt_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"Decision Tree model training completed.\")\n",
        "\n",
        "# Train set evaluation (predict on the training set)\n",
        "y_train_pred = dt_model.predict(X_train_scaled)\n",
        "\n",
        "# Generate classification report for the training set\n",
        "print(\"\\nTraining Set Classification Report:\")\n",
        "print(classification_report(y_train_encoded, y_train_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Test data batch processing\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = dt_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Test set evaluation (predict on the test set)\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy}\")\n",
        "print(\"\\nTest Set Classification Report:\")\n",
        "print(classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7vmjmHf3Qwu",
        "outputId": "935f2602-5da4-4fa4-8add-4fa8e734951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n",
            "Decision Tree model training completed.\n",
            "\n",
            "Training Set Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.99      1.00      1.00      1989\n",
            "     Textiles       0.99      1.00      0.99      1992\n",
            "         Wood       1.00      0.99      1.00      1966\n",
            "    cardboard       1.00      1.00      1.00      2194\n",
            "      e-waste       1.00      1.00      1.00      1924\n",
            "        glass       1.00      1.00      1.00      2014\n",
            "      medical       1.00      1.00      1.00      2154\n",
            "        metal       1.00      1.00      1.00      1982\n",
            "        paper       1.00      1.00      1.00      2199\n",
            "      plastic       1.00      1.00      1.00      2094\n",
            "\n",
            "     accuracy                           1.00     20508\n",
            "    macro avg       1.00      1.00      1.00     20508\n",
            " weighted avg       1.00      1.00      1.00     20508\n",
            "\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "\n",
            "Test Accuracy: 0.31337753510140404\n",
            "\n",
            "Test Set Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.14      0.22      0.17       497\n",
            "     Textiles       0.33      0.57      0.42       498\n",
            "         Wood       0.19      0.13      0.15       492\n",
            "    cardboard       0.45      0.35      0.39       548\n",
            "      e-waste       0.52      0.23      0.32       481\n",
            "        glass       0.45      0.37      0.41       504\n",
            "      medical       0.22      0.18      0.20       539\n",
            "        metal       0.33      0.32      0.33       496\n",
            "        paper       0.36      0.38      0.37       550\n",
            "      plastic       0.33      0.37      0.35       523\n",
            "\n",
            "     accuracy                           0.31      5128\n",
            "    macro avg       0.33      0.31      0.31      5128\n",
            " weighted avg       0.33      0.31      0.31      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train accuracy calculation (predict on the training set)\n",
        "train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy}\")"
      ],
      "metadata": {
        "id": "2n0w8WL-8CvT",
        "outputId": "c1c4bcd8-214e-42cc-e055-7b06f59c90e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9984396333138288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data in batches\n",
        "scaled_train_features = []\n",
        "for i in range(0, len(X_train), batch_size):\n",
        "    batch_X = X_train[i:i + batch_size]\n",
        "    # Fit and transform the batch data\n",
        "    scaled_batch = scaler.fit_transform(batch_X)\n",
        "    scaled_train_features.extend(scaled_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of training data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Stack all the scaled training features into a single array\n",
        "X_train_scaled = np.array(scaled_train_features)\n",
        "\n",
        "# Train the XGBoost model on the entire scaled training data\n",
        "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"XGBoost model training completed.\")\n",
        "\n",
        "# Training set predictions and evaluation\n",
        "y_train_pred = xgb_model.predict(X_train_scaled)\n",
        "train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy}\")\n",
        "print(\"\\nTrain Classification Report:\\n\", classification_report(y_train_encoded, y_train_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Test data batch processing\n",
        "all_y_pred = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = xgb_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Test set evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy}\")\n",
        "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keDvn4o8xYCU",
        "outputId": "f55e9dae-5f5b-4415-d4b8-c008becd072f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 of training data\n",
            "Processed batch 2 of training data\n",
            "Processed batch 3 of training data\n",
            "Processed batch 4 of training data\n",
            "Processed batch 5 of training data\n",
            "Processed batch 6 of training data\n",
            "Processed batch 7 of training data\n",
            "Processed batch 8 of training data\n",
            "Processed batch 9 of training data\n",
            "Processed batch 10 of training data\n",
            "Processed batch 11 of training data\n",
            "Processed batch 12 of training data\n",
            "Processed batch 13 of training data\n",
            "Processed batch 14 of training data\n",
            "Processed batch 15 of training data\n",
            "Processed batch 16 of training data\n",
            "Processed batch 17 of training data\n",
            "Processed batch 18 of training data\n",
            "Processed batch 19 of training data\n",
            "Processed batch 20 of training data\n",
            "Processed batch 21 of training data\n",
            "Processed batch 22 of training data\n",
            "Processed batch 23 of training data\n",
            "Processed batch 24 of training data\n",
            "Processed batch 25 of training data\n",
            "Processed batch 26 of training data\n",
            "Processed batch 27 of training data\n",
            "Processed batch 28 of training data\n",
            "Processed batch 29 of training data\n",
            "Processed batch 30 of training data\n",
            "Processed batch 31 of training data\n",
            "Processed batch 32 of training data\n",
            "Processed batch 33 of training data\n",
            "Processed batch 34 of training data\n",
            "Processed batch 35 of training data\n",
            "Processed batch 36 of training data\n",
            "Processed batch 37 of training data\n",
            "Processed batch 38 of training data\n",
            "Processed batch 39 of training data\n",
            "Processed batch 40 of training data\n",
            "Processed batch 41 of training data\n",
            "Processed batch 42 of training data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [18:55:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model training completed.\n",
            "\n",
            "Train Accuracy: 0.9555783109030622\n",
            "\n",
            "Train Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.86      0.88      0.87      1989\n",
            "     Textiles       0.88      0.89      0.88      1992\n",
            "         Wood       0.88      0.87      0.88      1966\n",
            "    cardboard       0.99      0.98      0.98      2194\n",
            "      e-waste       1.00      1.00      1.00      1924\n",
            "        glass       1.00      1.00      1.00      2014\n",
            "      medical       0.94      0.93      0.93      2154\n",
            "        metal       1.00      1.00      1.00      1982\n",
            "        paper       1.00      1.00      1.00      2199\n",
            "      plastic       1.00      1.00      1.00      2094\n",
            "\n",
            "     accuracy                           0.96     20508\n",
            "    macro avg       0.95      0.95      0.95     20508\n",
            " weighted avg       0.96      0.96      0.96     20508\n",
            "\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "\n",
            "Test Accuracy: 0.8771450858034321\n",
            "\n",
            "Test Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.94      0.98      0.96       497\n",
            "     Textiles       0.93      0.93      0.93       498\n",
            "         Wood       0.96      0.96      0.96       492\n",
            "    cardboard       0.85      0.93      0.89       548\n",
            "      e-waste       0.97      0.80      0.88       481\n",
            "        glass       0.88      0.80      0.84       504\n",
            "      medical       0.81      0.92      0.86       539\n",
            "        metal       0.90      0.74      0.81       496\n",
            "        paper       0.83      0.88      0.85       550\n",
            "      plastic       0.76      0.83      0.80       523\n",
            "\n",
            "     accuracy                           0.88      5128\n",
            "    macro avg       0.88      0.88      0.88      5128\n",
            " weighted avg       0.88      0.88      0.88      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Train the XGBoost model on the entire scaled training data\n",
        "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"XGBoost model training completed.\")\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Scale test data in batches\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Scale the test data in batches\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = xgb_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Convert to NumPy array for evaluation\n",
        "all_y_pred = np.array(all_y_pred)\n",
        "scaled_test_features = np.array(scaled_test_features)\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX9O13ALlhFu",
        "outputId": "233b5385-670c-410a-c040-50183727c243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [18:42:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.907371294851794\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      0.98      0.99       497\n",
            "     Textiles       0.99      0.99      0.99       498\n",
            "         Wood       0.99      0.95      0.97       492\n",
            "    cardboard       0.88      0.92      0.90       548\n",
            "      e-waste       0.91      0.91      0.91       481\n",
            "        glass       0.89      0.85      0.87       504\n",
            "      medical       0.89      0.93      0.91       539\n",
            "        metal       0.85      0.84      0.84       496\n",
            "        paper       0.89      0.87      0.88       550\n",
            "      plastic       0.82      0.84      0.83       523\n",
            "\n",
            "     accuracy                           0.91      5128\n",
            "    macro avg       0.91      0.91      0.91      5128\n",
            " weighted avg       0.91      0.91      0.91      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train accuracy for XG BOOST"
      ],
      "metadata": {
        "id": "uk9bHDB8NVjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Train the XGBoost model on the entire scaled training data\n",
        "xgb_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"XGBoost model training completed.\")\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_accuracy = xgb_model.score(X_train_scaled, y_train_encoded)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Scale test data in batches\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Scale the test data in batches\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = xgb_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Convert to NumPy array for evaluation\n",
        "all_y_pred = np.array(all_y_pred)\n",
        "scaled_test_features = np.array(scaled_test_features)\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsTvjwDfNNO_",
        "outputId": "07292217-fd34-407b-e4d4-e87e00e9a5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [21:36:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model training completed.\n",
            "Training Accuracy: 0.9332\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.907371294851794\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      0.98      0.99       497\n",
            "     Textiles       0.99      0.99      0.99       498\n",
            "         Wood       0.99      0.95      0.97       492\n",
            "    cardboard       0.88      0.92      0.90       548\n",
            "      e-waste       0.91      0.91      0.91       481\n",
            "        glass       0.89      0.85      0.87       504\n",
            "      medical       0.89      0.93      0.91       539\n",
            "        metal       0.85      0.84      0.84       496\n",
            "        paper       0.89      0.87      0.88       550\n",
            "      plastic       0.82      0.84      0.83       523\n",
            "\n",
            "     accuracy                           0.91      5128\n",
            "    macro avg       0.91      0.91      0.91      5128\n",
            " weighted avg       0.91      0.91      0.91      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1t4_qQR9Bie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize batch parameters\n",
        "batch_size = 500\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Initialize the AdaBoost model with default base estimator\n",
        "ada_model = AdaBoostClassifier(\n",
        "    n_estimators=100,  # Number of weak learners\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Train the AdaBoost model on the entire scaled training data\n",
        "ada_model.fit(X_train_scaled, y_train_encoded)\n",
        "print(\"AdaBoost model training completed.\")\n",
        "\n",
        "# Load and encode test data labels\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Scale test data in batches\n",
        "scaled_test_features = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Scale the test data in batches\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "    # Use the trained scaler from the training data for consistent scaling\n",
        "    scaled_batch = scaler.transform(batch_X)\n",
        "    scaled_test_features.extend(scaled_batch)\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = ada_model.predict(scaled_batch)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(5)  # Sleep for 5 seconds\n",
        "\n",
        "# Convert to NumPy array for evaluation\n",
        "all_y_pred = np.array(all_y_pred)\n",
        "scaled_test_features = np.array(scaled_test_features)\n",
        "\n",
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyve7RG5ryLa",
        "outputId": "4e4a4003-6187-47ce-8e02-5120fff68bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost model training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy: 0.6788221528861155\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.85      0.93      0.89       497\n",
            "     Textiles       0.94      0.91      0.92       498\n",
            "         Wood       0.90      0.93      0.92       492\n",
            "    cardboard       0.75      0.66      0.70       548\n",
            "      e-waste       0.74      0.80      0.77       481\n",
            "        glass       0.52      0.72      0.61       504\n",
            "      medical       0.64      0.50      0.56       539\n",
            "        metal       0.58      0.37      0.45       496\n",
            "        paper       0.51      0.64      0.57       550\n",
            "      plastic       0.42      0.38      0.40       523\n",
            "\n",
            "     accuracy                           0.68      5128\n",
            "    macro avg       0.68      0.68      0.68      5128\n",
            " weighted avg       0.68      0.68      0.67      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wQ2hkQ1g1WI3",
        "outputId": "91946c67-15e3-4fe5-a9be-82cfb5b91887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "AdaBoostClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-59825fb39fdc>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Initialize the AdaBoost model with a decision stump as the base estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m ada_model = AdaBoostClassifier(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Weak learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: AdaBoostClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_accuracy = accuracy_score(y_test_encoded, all_y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, all_y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Calculate predicted probabilities\n",
        "y_pred_prob = xgb_model.predict_proba(scaled_test_features)\n",
        "\n",
        "# Calculate variance of predicted probabilities for each class\n",
        "variance_pred_prob = np.var(y_pred_prob, axis=0)\n",
        "print(\"\\nVariance of Predicted Probabilities for each class:\\n\", variance_pred_prob)\n",
        "\n",
        "# Calculate variance of predicted classes\n",
        "variance_pred_classes = np.var(all_y_pred)\n",
        "print(f\"\\nVariance of Predicted Classes: {variance_pred_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpYmCoSEvUuE",
        "outputId": "c3e21a91-95f0-47ee-f072-629714ca8f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6788221528861155\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.85      0.93      0.89       497\n",
            "     Textiles       0.94      0.91      0.92       498\n",
            "         Wood       0.90      0.93      0.92       492\n",
            "    cardboard       0.75      0.66      0.70       548\n",
            "      e-waste       0.74      0.80      0.77       481\n",
            "        glass       0.52      0.72      0.61       504\n",
            "      medical       0.64      0.50      0.56       539\n",
            "        metal       0.58      0.37      0.45       496\n",
            "        paper       0.51      0.64      0.57       550\n",
            "      plastic       0.42      0.38      0.40       523\n",
            "\n",
            "     accuracy                           0.68      5128\n",
            "    macro avg       0.68      0.68      0.68      5128\n",
            " weighted avg       0.68      0.68      0.67      5128\n",
            "\n",
            "\n",
            "Variance of Predicted Probabilities for each class:\n",
            " [0.06312153 0.06210924 0.05824703 0.07516632 0.06790954 0.06275208\n",
            " 0.06133926 0.05661442 0.06349912 0.05839079]\n",
            "\n",
            "Variance of Predicted Classes: 8.278387134900372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load training and test data\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category'])\n",
        "y_train = train_data['Category']\n",
        "\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category'])\n",
        "y_test = test_data['Category']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define base classifiers and their parameter grids\n",
        "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "param_grid_tree = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
        "param_grid_log_reg = {'C': [0.1, 1, 10]}\n",
        "\n",
        "# Hyperparameter tuning for each model\n",
        "svm_clf = GridSearchCV(SVC(probability=True), param_grid_svm, cv=3).fit(X_train, y_train).best_estimator_\n",
        "tree_clf = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=3).fit(X_train, y_train).best_estimator_\n",
        "log_reg_clf = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_log_reg, cv=3).fit(X_train, y_train).best_estimator_\n",
        "\n",
        "# Initialize Voting Classifier with soft voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('svm', svm_clf), ('tree', tree_clf), ('log_reg', log_reg_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Batch-wise prediction on the test set\n",
        "batch_size = 500  # Define a suitable batch size\n",
        "n_batches = int(np.ceil(len(X_test) / batch_size))\n",
        "y_pred = []\n",
        "\n",
        "for i in range(n_batches):\n",
        "    start = i * batch_size\n",
        "    end = min((i + 1) * batch_size, len(X_test))\n",
        "    batch_pred = voting_clf.predict(X_test[start:end])\n",
        "    y_pred.extend(batch_pred)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ktY1tIJ_7SUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import time\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train features\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Load the test features\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Define individual models\n",
        "svm_model = make_pipeline(MinMaxScaler(), SVC(C=1, kernel='linear', probability=True))\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Create a Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('svm', svm_model),\n",
        "    ('xgb', xgb_model)\n",
        "], voting='soft')  # Using soft voting to consider predicted probabilities\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "print(\"Voting Classifier training completed.\")\n",
        "\n",
        "# Batch size for processing\n",
        "batch_size = 500\n",
        "all_y_pred = []\n",
        "\n",
        "# Test data batch processing\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = voting_clf.predict(batch_X)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(1)  # Optional sleep to prevent overloading resources\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy for Voting Classifier: {test_accuracy:.4f}\")\n",
        "print(classification_report(y_test, all_y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d9gK99c8Xwt",
        "outputId": "425bca32-c481-4381-81bc-66c3fa8dd78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:33:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy for Voting Classifier: 0.9288\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      0.99      0.99       497\n",
            "     Textiles       0.99      0.99      0.99       498\n",
            "         Wood       0.99      0.98      0.99       492\n",
            "    cardboard       0.91      0.94      0.93       548\n",
            "      e-waste       0.93      0.94      0.93       481\n",
            "        glass       0.90      0.89      0.89       504\n",
            "      medical       0.93      0.92      0.92       539\n",
            "        metal       0.90      0.88      0.89       496\n",
            "        paper       0.91      0.90      0.90       550\n",
            "      plastic       0.85      0.87      0.86       523\n",
            "\n",
            "     accuracy                           0.93      5128\n",
            "    macro avg       0.93      0.93      0.93      5128\n",
            " weighted avg       0.93      0.93      0.93      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import time\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train features\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Load the test features\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Define individual models\n",
        "svm_model = make_pipeline(MinMaxScaler(), SVC(C=1, kernel='linear', probability=True))\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Create a Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('svm', svm_model),\n",
        "    ('xgb', xgb_model)\n",
        "], voting='soft')  # Using soft voting to consider predicted probabilities\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "print(\"Voting Classifier training completed.\")\n",
        "\n",
        "# Batch size for processing\n",
        "batch_size = 500\n",
        "all_y_pred = []\n",
        "\n",
        "# Test data batch processing\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = voting_clf.predict(batch_X)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "    time.sleep(1)  # Optional sleep to prevent overloading resources\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy for Voting Classifier: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, all_y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqSTUzAw5P7t",
        "outputId": "9f25313d-34f2-41ff-c98b-de98741bd337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [19:31:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier training completed.\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy for Voting Classifier: 0.9288\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      0.99      0.99       497\n",
            "     Textiles       0.99      0.99      0.99       498\n",
            "         Wood       0.99      0.98      0.99       492\n",
            "    cardboard       0.91      0.94      0.93       548\n",
            "      e-waste       0.93      0.94      0.93       481\n",
            "        glass       0.90      0.89      0.89       504\n",
            "      medical       0.93      0.92      0.92       539\n",
            "        metal       0.90      0.88      0.89       496\n",
            "        paper       0.91      0.90      0.90       550\n",
            "      plastic       0.85      0.87      0.86       523\n",
            "\n",
            "     accuracy                           0.93      5128\n",
            "    macro avg       0.93      0.93      0.93      5128\n",
            " weighted avg       0.93      0.93      0.93      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training accuracy\n",
        "train_accuracy = voting_clf.score(X_train, y_train)\n",
        "print(f\"Training Accuracy for Voting Classifier: {train_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvLxt15j9C3a",
        "outputId": "09c4ff10-cded-4a3d-e941-1cdf530c9f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy for Voting Classifier: 0.9335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the training classification report\n",
        "train_y_pred = voting_clf.predict(X_train)\n",
        "print(\"Training Classification Report:\")\n",
        "print(classification_report(y_train, train_y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3JcF_bx9UhU",
        "outputId": "bdc947ce-b12f-414c-fddb-7d73419ff480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.82      0.82      0.82      1989\n",
            "     Textiles       0.82      0.83      0.82      1992\n",
            "         Wood       0.83      0.81      0.82      1966\n",
            "    cardboard       0.96      0.96      0.96      2194\n",
            "      e-waste       1.00      1.00      1.00      1924\n",
            "        glass       1.00      1.00      1.00      2014\n",
            "      medical       0.90      0.90      0.90      2154\n",
            "        metal       1.00      1.00      1.00      1982\n",
            "        paper       1.00      1.00      1.00      2199\n",
            "      plastic       1.00      1.00      1.00      2094\n",
            "\n",
            "     accuracy                           0.93     20508\n",
            "    macro avg       0.93      0.93      0.93     20508\n",
            " weighted avg       0.93      0.93      0.93     20508\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train features\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Load the test features\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Define batch size for processing\n",
        "batch_size = 500\n",
        "\n",
        "# Define the SVM model pipeline with MinMax scaling and specified C value\n",
        "svm_model = make_pipeline(MinMaxScaler(), SVC(C=0.1, kernel='linear', max_iter=5000))\n",
        "\n",
        "# Train the SVM model on the entire training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_accuracy = svm_model.score(X_train, y_train)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Test data batch processing\n",
        "all_y_pred = []\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "mhYB2w2JCZ8K",
        "outputId": "6fd3f046-e9cf-4c42-992c-94c181f263b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8828\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-81ffb5f770c0>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Predict on the current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my_pred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mall_y_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch_X = X_test[i:i + batch_size]\n",
        "\n",
        "    # Predict on the current batch\n",
        "    y_pred_batch = svm_model.predict(batch_X)\n",
        "    all_y_pred.extend(y_pred_batch)\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "print(f\"Test Accuracy for C=0.1: {test_accuracy:.4f}\")\n",
        "\n",
        "# Generate the classification report for the test set\n",
        "classification_rep = classification_report(y_test, all_y_pred)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PAfcUgEERwY",
        "outputId": "5dc6a757-ad23-4d17-f5e6-87d501780a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for C=0.1: 0.9142\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       1.00      1.00      1.00       497\n",
            "     Textiles       0.99      0.99      0.99       498\n",
            "         Wood       0.99      0.98      0.98       492\n",
            "    cardboard       0.90      0.92      0.91       548\n",
            "      e-waste       0.91      0.94      0.93       481\n",
            "        glass       0.86      0.88      0.87       504\n",
            "      medical       0.93      0.92      0.93       539\n",
            "        metal       0.87      0.86      0.87       496\n",
            "        paper       0.88      0.85      0.87       550\n",
            "      plastic       0.82      0.81      0.81       523\n",
            "\n",
            "     accuracy                           0.91      5128\n",
            "    macro avg       0.92      0.92      0.92      5128\n",
            " weighted avg       0.91      0.91      0.91      5128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train features\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Load the test features\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Batch size for processing\n",
        "batch_size = 500\n",
        "\n",
        "# Define a list of C values to try\n",
        "C_values = [0.01, 0.1, 1]  # Adjusted to avoid very low C values\n",
        "\n",
        "# Store results for each C value\n",
        "results = {}\n",
        "\n",
        "# Iterate over each C value\n",
        "for C in C_values:\n",
        "    print(f\"\\nTraining with C={C}...\")\n",
        "\n",
        "    # Define the SVM model pipeline with MinMax scaling and specified C value\n",
        "    final_svm = make_pipeline(MinMaxScaler(), SVC(C=C, kernel='linear', max_iter=5000))\n",
        "\n",
        "    # Train the SVM model on the entire training data\n",
        "    final_svm.fit(X_train, y_train)\n",
        "    print(\"Model training completed.\")\n",
        "\n",
        "    # Calculate and print training accuracy\n",
        "    train_accuracy = final_svm.score(X_train, y_train)\n",
        "    print(f\"Training Accuracy for C={C}: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Test data batch processing\n",
        "    all_y_pred = []\n",
        "\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        batch_X = X_test[i:i + batch_size]\n",
        "\n",
        "        # Predict on the current batch\n",
        "        y_pred_batch = final_svm.predict(batch_X)\n",
        "        all_y_pred.extend(y_pred_batch)\n",
        "\n",
        "        print(f\"Processed batch {i // batch_size + 1} of test data\")\n",
        "        time.sleep(2)  # Optional sleep to prevent overloading resources\n",
        "\n",
        "    # Evaluation\n",
        "    test_accuracy = accuracy_score(y_test, all_y_pred)\n",
        "    print(f\"Test Accuracy for C={C}: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[C] = {\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'classification_report': classification_report(y_test, all_y_pred, output_dict=True)\n",
        "    }\n",
        "\n",
        "# Find the best C value based on the highest test accuracy\n",
        "best_C = max(results, key=lambda x: results[x]['test_accuracy'])\n",
        "best_accuracy = results[best_C]['test_accuracy']\n",
        "\n",
        "print(f\"\\nBest C value: {best_C}\")\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Final results summary\n",
        "print(\"\\nSummary of Results:\")\n",
        "for C, result in results.items():\n",
        "    print(f\"C={C}: Training Accuracy={result['train_accuracy']:.4f}, Test Accuracy={result['test_accuracy']:.4f}\")\n",
        "    print(\"Classification Report:\\n\", result['classification_report'])\n",
        "\n",
        "# Classify a random test sample with the best model\n",
        "# Reinitialize the best model with the best C value and train it\n",
        "print(\"\\nClassifying a random test sample using the best model...\")\n",
        "final_svm = make_pipeline(MinMaxScaler(), SVC(C=best_C, kernel='linear', max_iter=5000))\n",
        "final_svm.fit(X_train, y_train)  # Re-train model with the best C if not already trained\n",
        "\n",
        "# Randomly select an index from the test set\n",
        "random_index = random.randint(0, len(X_test) - 1)\n",
        "random_sample = X_test[random_index].reshape(1, -1)  # Reshape to 2D array for prediction\n",
        "\n",
        "# Predict the category for the randomly selected sample\n",
        "predicted_category = final_svm.predict(random_sample)[0]\n",
        "actual_category = y_test[random_index]\n",
        "\n",
        "print(f\"\\nRandom Test Sample Index: {random_index}\")\n",
        "print(f\"Predicted Category: {predicted_category}\")\n",
        "print(f\"Actual Category: {actual_category}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6L0EF1WE9pe",
        "outputId": "c0a9ecdc-ed6b-4a80-f7a8-463c0e105a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with C=0.01...\n",
            "Model training completed.\n",
            "Training Accuracy for C=0.01: 0.8362\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy for C=0.01: 0.8910\n",
            "\n",
            "Training with C=0.1...\n",
            "Model training completed.\n",
            "Training Accuracy for C=0.1: 0.8828\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy for C=0.1: 0.9142\n",
            "\n",
            "Training with C=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n",
            "Training Accuracy for C=1: 0.9205\n",
            "Processed batch 1 of test data\n",
            "Processed batch 2 of test data\n",
            "Processed batch 3 of test data\n",
            "Processed batch 4 of test data\n",
            "Processed batch 5 of test data\n",
            "Processed batch 6 of test data\n",
            "Processed batch 7 of test data\n",
            "Processed batch 8 of test data\n",
            "Processed batch 9 of test data\n",
            "Processed batch 10 of test data\n",
            "Processed batch 11 of test data\n",
            "Test Accuracy for C=1: 0.9122\n",
            "\n",
            "Training with C=10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "6XJNaK9dLtGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Paths to feature files\n",
        "train_features_path = '/content/drive/My Drive/combined_features_labels.csv'\n",
        "test_features_path = '/content/drive/My Drive/test_features.csv'\n",
        "\n",
        "# Load the train features\n",
        "train_data = pd.read_csv(train_features_path)\n",
        "X_train = train_data.drop(columns=['Category']).values\n",
        "y_train = train_data['Category'].values\n",
        "\n",
        "# Load the test features\n",
        "test_data = pd.read_csv(test_features_path)\n",
        "X_test = test_data.drop(columns=['Category']).values\n",
        "y_test = test_data['Category'].values\n",
        "\n",
        "# Set C value\n",
        "C = 1\n",
        "print(f\"\\nTraining with C={C}...\")\n",
        "\n",
        "# Define the SVM model pipeline with MinMax scaling and specified C value\n",
        "final_svm = make_pipeline(MinMaxScaler(), SVC(C=C, kernel='linear', max_iter=5000))\n",
        "\n",
        "# Train the SVM model on the entire training data\n",
        "final_svm.fit(X_train, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Calculate and print training accuracy\n",
        "train_accuracy = final_svm.score(X_train, y_train)\n",
        "print(f\"Training Accuracy for C={C}: {train_accuracy:.4f}\")\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = final_svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy for C={C}: {test_accuracy:.4f}\")\n",
        "\n",
        "# Generate the classification report for the test set\n",
        "classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "# Optionally, you can print the detailed report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcQDvHZRLpXt",
        "outputId": "d7da7124-0600-4694-8948-e99a538b1158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with C=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n",
            "Training Accuracy for C=1: 0.9205\n",
            "Test Accuracy for C=1: 0.9122\n",
            "Classification Report:\n",
            " {'Organic Waste': {'precision': 0.993963782696177, 'recall': 0.993963782696177, 'f1-score': 0.993963782696177, 'support': 497.0}, 'Textiles': {'precision': 0.9919028340080972, 'recall': 0.9839357429718876, 'f1-score': 0.9879032258064516, 'support': 498.0}, 'Wood': {'precision': 0.9798387096774194, 'recall': 0.9878048780487805, 'f1-score': 0.9838056680161943, 'support': 492.0}, 'cardboard': {'precision': 0.8854166666666666, 'recall': 0.9306569343065694, 'f1-score': 0.9074733096085409, 'support': 548.0}, 'e-waste': {'precision': 0.899009900990099, 'recall': 0.9438669438669439, 'f1-score': 0.920892494929006, 'support': 481.0}, 'glass': {'precision': 0.847036328871893, 'recall': 0.878968253968254, 'f1-score': 0.8627069133398247, 'support': 504.0}, 'medical': {'precision': 0.9148148148148149, 'recall': 0.9165120593692022, 'f1-score': 0.9156626506024096, 'support': 539.0}, 'metal': {'precision': 0.8872340425531915, 'recall': 0.8407258064516129, 'f1-score': 0.8633540372670807, 'support': 496.0}, 'paper': {'precision': 0.8943396226415095, 'recall': 0.8618181818181818, 'f1-score': 0.8777777777777778, 'support': 550.0}, 'plastic': {'precision': 0.8370221327967807, 'recall': 0.7954110898661568, 'f1-score': 0.8156862745098039, 'support': 523.0}, 'accuracy': 0.9122464898595943, 'macro avg': {'precision': 0.913057883571665, 'recall': 0.9133663673363766, 'f1-score': 0.9129226134553268, 'support': 5128.0}, 'weighted avg': {'precision': 0.9121273986262564, 'recall': 0.9122464898595943, 'f1-score': 0.9118963755208191, 'support': 5128.0}}\n",
            "\n",
            "Detailed Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Organic Waste       0.99      0.99      0.99       497\n",
            "     Textiles       0.99      0.98      0.99       498\n",
            "         Wood       0.98      0.99      0.98       492\n",
            "    cardboard       0.89      0.93      0.91       548\n",
            "      e-waste       0.90      0.94      0.92       481\n",
            "        glass       0.85      0.88      0.86       504\n",
            "      medical       0.91      0.92      0.92       539\n",
            "        metal       0.89      0.84      0.86       496\n",
            "        paper       0.89      0.86      0.88       550\n",
            "      plastic       0.84      0.80      0.82       523\n",
            "\n",
            "     accuracy                           0.91      5128\n",
            "    macro avg       0.91      0.91      0.91      5128\n",
            " weighted avg       0.91      0.91      0.91      5128\n",
            "\n"
          ]
        }
      ]
    }
  ]
}